# RAG (Retrieval-Augmented Generation) API

## Overview

The **RAG API** is a powerful document retrieval and response generation system built using **LangChain**, **FAISS**, and **Deepseek** model. This API allows you to upload, index, search, and retrieve information from a variety of documents (PDF, images, text files, etc.). Once the relevant data is retrieved, it uses a language model to generate contextually accurate and informative responses based on the input query.

The RAG (Retrieval-Augmented Generation) architecture enhances the capabilities of traditional search engines by combining document retrieval with advanced text generation, ensuring responses are more informative and relevant.

### Key Features:
- **Document Ingestion**: The API accepts various document formats, including PDF, images, scanned files and text files.
- **FAISS Indexing**: Utilizes FAISS for fast, efficient vector search.
- **LangChain for Response Generation**: Integration with LangChain allows for generating answers based on retrieved documents using Deepseek model.
- **Flexible Document Processing**: Supports hybrid document extraction using both traditional text extraction and Optical Character Recognition (OCR) for image-based PDFs.

---

## ðŸ“º Project Demo

Watch the full walkthrough and explanation on YouTube: 

[![RAG API Project Demo](https://img.youtube.com/vi/qZ3hS1Bzc3I/hqdefault.jpg)](https://www.youtube.com/watch?v=qZ3hS1Bzc3I)

## Requirements

This project relies on several Python libraries. To install the necessary dependencies, please ensure you have Python 3.8+ installed.

### Dependencies:

- `fastapi` - Fast API framework for creating the RESTful API.
- `uvicorn` - ASGI server to run the FastAPI app.
- `PyMuPDF` - For extracting text from PDF files.
- `pytesseract` - Optical Character Recognition (OCR) for extracting text from images.
- `sentence-transformers` - For generating document embeddings.
- `faiss-cpu` - A library for efficient similarity search and clustering of dense vectors.
- `transformers` - For integrating Hugging Face models like GPT.
- `langchain` - For building and managing LLM-based applications.
- `python-dotenv` - To manage environment variables.
- `pydantic` - For data validation in FastAPI.
- `openai` - To use functionality of ChatOpenAI() object.
- `scikit-learn` - For additional utilities like cosine similarity.
- `gitpython`, `shutil`, and `pathlib` - For repo management and file operations.


## Models Used

### 1. **LLM Model: `deepseek/deepseek-chat-v3-0324`**
- **Description**: This model is responsible for answering the final query by generating responses based on the retrieved documents.
- **Hosting**: The model is hosted on [OpenRouter.ai](https://openrouter.ai), and it requires an API key for usage.
- **API Key**: You need to provide an API key from OpenRouter.ai to access the model. The free tier offers up to **50 requests per day**.

### 2. **Embedding Model: `SentenceTransformer("BAAI/bge-large-en-v1.5")`**
- **Description**: This model is used for generating document embeddings, which are then stored in a vector database (FAISS) for fast retrieval.
- **Hosting**: This model is downloaded and used locally in the environment.
- **Usage**: The embeddings generated by this model are used to match documents against the user's query to retrieve the most relevant information.


## How to Run
### 1. Clone the Repository:
Start by cloning this repository to your local machine:

```bash
git clone https://github.com/Web-hacker/RAG-API.git
cd RAG-API
```
## 2. Set up DocumentStore and VectorStore directory:
Create *data* and *index* directory by using following command:

```bash
mkdir data index
```
- **data** directory: This is where uploaded documents and cloned github repos are going to be stored.
- **index** directory: This is where indexed vectorstore is going to be stored.

### 3. Set Up Environment Variables:
Create a *.env* file in the app folder of the project and add the following environment variables:

```plaintext
OPENAI_API_KEY=your-openrouter-api-key
```
If you're using any other external APIs or credentials, you should add them here as well.

### 4. Install the dependencie
To install the required dependencies, create a virtual environment and run the following command:

```bash
pip install -r requirements.txt

```
### 5. Run the API:
You can start the API server using Uvicorn (ASGI server). Use this command when you're in RAG-API directory. Do not *cd* in *app directory*:

```bash
uvicorn app.api:app --reload-dir app
```
The API will be available at http://localhost:8000.

## API Endpoints

### 1. Root Endpoint
- **Endpoint**: /
- **Method**: GET
- **Description**: A simple health check endpoint that confirms the API is running.
- **Response**:
```json
{
  "message": "GS RAG Agent is running!"
}
```
### 2. File Upload Endpoint
- **Endpoint**: /upload
- **Method**: POST
- **Description**: Uploads a file and ingests its content into the vector database.
- **Request Body**: multipart/form-data
    - **file**: The file to be uploaded (PDF, Image, or Text).
- **Response**:
```json
{
  "message": "File 'your-file.pdf' saved and ingested successfully."
}
```
### 3. Repo Clone & Ingest Endpoint
- **Endpoint**: /clone
- **Method**: POST
- **Description**: Clones a GitHub repository and ingests its contents into the vector database.
- **Request Body**:
```json
{
  "repo_url": "https://github.com/your/repo",
  "branch": "main"
}
```
- **Response**:
```json
{
  "message": "Repo cloned and ingested successfully at 'path/to/repo'"
}
```

### 4. RAG Query Endpoint
- **Endpoint**: /query
- **Method**: POST
- **Description**: Accepts a user query and returns an answer using the RAG pipeline.
- **Request Body**:
```json
{
  "query": "What is the capital of France?"
}
```
- **Response**:
```json
{
  "answer": "The capital of France is Paris.",
  "sources": ["source_file_1", "source_file_2"]
}
```

## Example Usage

### Uploading a Document:
```bash
curl -X 'POST' \
  'http://localhost:8000/upload' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@your-document.pdf'
```

### Cloning and Ingesting a Repository:
```bash
curl -X 'POST' \
  'http://localhost:8000/clone' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{"repo_url": "https://github.com/your/repo", "branch": "main"}'
```

### Asking a Question:
```bash
curl -X 'POST' \
  'http://localhost:8000/query' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{"query": "What is the capital of France?"}'
```

## Special Notes and Limitations

### Notes:
- **File Size**: Large documents, especially PDFs and images, may take some time to process and extract text. Ensure the files are of manageable size for optimal performance.

- **OCR Accuracy**: For PDFs containing images (e.g., scanned documents), OCR may not always be perfect. It depends on the quality and clarity of the image.

- **Indexing Limit**: The current version of the system stores documents in-memory (or a file-based index). For large-scale deployments, additional optimization or storage strategies (e.g., a database) may be required.

### Limitations:
- **Openrouter API**: This project relies on Openrouter's hosted deepseek models. Ensure you have an API key and your usage is within the limits of the Openrouter API.

- **Hardware Resources**: Text embedding and search can be resource-intensive, especially for large documents and queries. Ensure the machine running this service has adequate memory and processing power.

### Future Improvements
- **Multi-language Support**: Add support for documents in multiple languages, including automatic language detection.

- **Better Summarization**: Implement hierarchical summarization techniques for more accurate and concise answers.

- **Caching and Persistence**: Implement caching mechanisms for frequent queries and document retrieval, reducing response times.

- **Scaling**: Deploy the system on a cloud platform for scaling and better performance.

## Contributing
Feel free to contribute to this project by submitting issues or pull requests. If you'd like to improve the system, whether it's better performance, new features, or bug fixes, all contributions are welcome!
